{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a975d96b-2d6a-43c8-8c4d-99b34c2f73b2",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries  \n",
    "First, we need to import the required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdd4277-3a44-457b-8111-1810f24e1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d68f99-aa52-48f6-8c95-215a86a40fa0",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "pandas and numpy: Libraries for data manipulation.\n",
    "\n",
    "train_test_split: To split the dataset into training and testing sets.\n",
    "\n",
    "StandardScaler, OneHotEncoder: Tools for scaling and encoding.\n",
    "\n",
    "SimpleImputer: Handles missing values.\n",
    "\n",
    "Pipeline: Organizes preprocessing steps into a sequential structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7189aee-cb46-4d3c-992a-852db01d049a",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n",
    "We'll load the Heart Disease dataset from the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb62988-3e7a-4177-a4b2-289be87957bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "columns = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \n",
    "    \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"  # 'num' is the target\n",
    "]\n",
    "\n",
    "# Load dataset with '?' as missing values (NaN)\n",
    "df = pd.read_csv(url, names=columns, na_values=\"?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bd599-1bca-4fd6-bfad-44f4f4fa1649",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The dataset contains clinical data to predict heart disease.\n",
    "\n",
    "na_values=\"?\" ensures that any '?' in the data is treated as a missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df3c86-a352-48ba-bd56-a0e44120d7dc",
   "metadata": {},
   "source": [
    "### Step 3: Handle Missing Values\n",
    "We'll impute missing values for two important features using the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e43da-2df6-40d9-b93a-3f4d4c5bb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ca'].fillna(df['ca'].median(), inplace=True)  # 'ca': number of major vessels\n",
    "df['thal'].fillna(df['thal'].median(), inplace=True)  # 'thal': thalassemia status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fd3d2-ae9a-4cfe-ab4f-9a38843b007d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "ca and thal contain some missing values, which we replace with their median.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919aa97-64d7-4fa7-bbdb-5e74f13cc233",
   "metadata": {},
   "source": [
    "### Step 4: Convert Target Variable to Binary\n",
    "The original target variable num contains multiple values. We convert it to binary (0: no heart disease, 1: heart disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0a44c4-8454-4760-a71d-4e5d675ffe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num'] = df['num'].apply(lambda x: 1 if x > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60da0a-411d-4377-8803-d014d53a66c5",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Heart disease presence: 1\n",
    "No heart disease: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f695f-8718-4957-b3be-6ede27a3f147",
   "metadata": {},
   "source": [
    "### Step 5: Define Features and Target Variables\n",
    "We'll separate the features and target variable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a17a7ff1-13d9-4af6-a112-de7e9b8bb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('num', axis=1)  # Features\n",
    "y = df['num']  # Target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d536537-6c98-474c-ace1-54c7dd50ad8e",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "X: All features except the target (num).\n",
    "y: The binary target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcdccfb-ecd1-4caf-8921-ba3ae8f185bd",
   "metadata": {},
   "source": [
    "### Step 6: Encode Categorical Variables and Scale Numerical Features\n",
    "We'll apply one-hot encoding to categorical variables and scale numerical features.\n",
    "### Step 6.1: Identify Categorical and Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe8ddde-c583-49c8-b796-ce6710ea0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c995ba-03aa-4f48-a0f1-2ed1ef805dc7",
   "metadata": {},
   "source": [
    "### Step 6.2: Create Preprocessing Pipelines\n",
    "Pipeline for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7acc001b-788b-48c8-ab9b-241f5cecc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing with median\n",
    "    ('scaler', StandardScaler())  # Scale numerical features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae03644-35ee-4e67-9090-46c47254bf3e",
   "metadata": {},
   "source": [
    "Pipeline for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df43da54-86b6-4294-9ea4-070334fc3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing with mode\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f047cdc-34fe-40bd-befe-773906d08d78",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Numerical Pipeline:\n",
    "\n",
    "Impute missing values with the median.\n",
    "\n",
    "Scale features with StandardScaler to normalize them.\n",
    "\n",
    "Categorical Pipeline:\n",
    "\n",
    "Impute missing values with the most frequent value.\n",
    "\n",
    "Apply one-hot encoding to categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffef3f-3384-4fd2-b2b1-40493327142c",
   "metadata": {},
   "source": [
    "### Step 6.3: Combine Pipelines Using ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524b1115-6110-41f9-bdd1-130ce66b6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ddef2-bac3-4870-b29a-60f84fa60d2d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The ColumnTransformer applies different transformations to numerical and categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cd365-a421-4494-909d-9f01ac419e59",
   "metadata": {},
   "source": [
    "### Step 7: Fit and Transform the Data\n",
    "We now fit the preprocessing pipelines to the data and transform the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "699578c0-9ba0-45c9-a0ce-cd74a56051b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d16c7-c287-427c-952f-d524c98bb287",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The transformed dataset is now ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c69956-5f6d-4384-8a9e-44d1ade8d19c",
   "metadata": {},
   "source": [
    "### Step 8: Split the Dataset into Training and Testing Sets\n",
    "We'll split the processed data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2fa00d-df5b-4f0b-be2c-71aa33cb633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c16387-b440-476c-a596-238b11ef99b0",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "80% of the data is used for training, and 20% for testing.\n",
    "\n",
    "stratify=y ensures that both sets maintain the same class distribution as the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b21eaf-0706-49ec-aa61-96bc6bd4db63",
   "metadata": {},
   "source": [
    "### Step 9: Display Dataset Shapes\n",
    "Finally, we'll print the shapes of the training and testing sets to verify our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59bb7fdc-bab6-4a24-b814-34322af3847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (242, 28)\n",
      "Testing set shape: (61, 28)\n",
      "Training labels shape: (242,)\n",
      "Testing labels shape: (61,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2042e05-f103-4623-b185-689cc14626e1",
   "metadata": {},
   "source": [
    "### Summary\n",
    "In this tutorial, we walked through the complete preprocessing pipeline for the Heart Disease dataset. \n",
    "The following steps were covered:\n",
    "\n",
    ">Handling missing values using median and most frequent value imputation.\n",
    "\n",
    ">Encoding categorical variables using one-hot encoding.\n",
    "\n",
    ">Scaling numerical variables with StandardScaler.\n",
    "\n",
    ">Splitting the dataset into training and testing sets using train_test_split.\n",
    "\n",
    ">This preprocessed dataset is now ready to be used with machine learning algorithms for building predictive models. \n",
    "\n",
    ">You can further explore different algorithms like logistic regression, decision trees, or neural networks on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260a801-c910-47a6-bb89-75e157a8eae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
